{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca1ce3e9-5fab-4ffc-885c-962e6d44cbb9",
   "metadata": {},
   "source": [
    "# Importing the necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac12e8ab-b5bc-4645-8daa-d9e0393e5ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import mlflow.pyfunc\n",
    "import mlflow.pytorch\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "import nannyml as nml\n",
    "import copy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baaad3b-7ba9-44c9-8010-3d6b0e64339a",
   "metadata": {},
   "source": [
    "# Importing data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc2729c-01fe-4e8d-bd38-3591bd7d0361",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/cpu_data_custom_orangepi.csv')\n",
    "# Convert string representation of lists back to actual lists\n",
    "df['features'] = df['features'].apply(ast.literal_eval)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bdb00d-709d-4206-96f3-bed8204f4229",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ef2815-f5e7-4f4e-ac81-936f05d45dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(df[:5000].index, df['target'][:5000])\n",
    "plt.xlabel('Record index')\n",
    "plt.ylabel('Usage value (%)')\n",
    "# plt.title('CPU Usage (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44754697-d01b-478d-bb0f-ac471c2ff3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        lstm_out, _ = self.lstm(input.view(len(input), 1, -1))\n",
    "        output = self.fc(lstm_out.view(len(input), -1))\n",
    "        return output[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a73be61-7d1c-4c0a-a1d9-027de46a44b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataframe.iloc[idx]\n",
    "        x = torch.tensor(sample['features'], dtype=torch.float32)\n",
    "        y = torch.tensor(sample['target'], dtype=torch.float32)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b63a02-fabe-4881-8c25-1522c8ad0c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer, epochs=10):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "    # print(f'Last Epoch {epoch+1}, Loss: {running_loss/len(dataloader)}')\n",
    "    print('======== Model Training completed ========')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe245c3-b19f-478f-8d85-5d49726b97e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_model(model, dataloader, criterion, optimizer, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        model.train()  # Set the model to training mode\n",
    "        \n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            inputs, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "    # print(f'Last Epoch {epoch+1}, Loss: {running_loss/len(dataloader)}')\n",
    "    print('======== Model Fine-tuning  completed ========')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f817591-78a3-4fed-a8e8-af0e4a156635",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 8  # Input size is the length of the input list\n",
    "hidden_size = 16\n",
    "output_size = 1\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76c2d57-3bdf-4190-83dc-9a17a67bf91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Need to add comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92105884-3603-4687-97c6-4e5c9a52840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df[0:60000]\n",
    "finetune_data = df[60000:70000]\n",
    "test_data = df[70000:75000] # this data will be used for initial DLE training\n",
    "prod_data = df [75000:]\n",
    "\n",
    "X_test=test_data[['features']]\n",
    "y_test=test_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a81d8b-14c7-43b4-bcec-c2e7a5673566",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "train_model(model, train_dataloader, criterion, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6751cda4-2465-4246-8724-12632975567c",
   "metadata": {},
   "source": [
    "# Pushing pretrained model to the model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ca0157-1be7-41f0-aeda-107b3c62241f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri('http://localhost:5000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3661cdfa-2d9b-491a-9d7f-0af3e69d0644",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment('Chapter-12')\n",
    "def log_models(model,modelname):\n",
    "    with mlflow.start_run() as run:\n",
    "        mlflow.pytorch.log_model(model, modelname)\n",
    "        # Register the model\n",
    "        model_uri = f\"runs:/{run.info.run_id}/{modelname}\"\n",
    "        mlflow.register_model(model_uri, \"Ch12Models\")\n",
    "log_models(model,\"ch12_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d26e7dd-fd69-4ed2-ae4e-4925cf1a674c",
   "metadata": {},
   "source": [
    "# Finetune and store updated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebee8af-4edb-45ed-855b-d8075fc889a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_dataset = CustomDataset(finetune_data)\n",
    "finetune_dataloader = DataLoader(finetune_dataset, batch_size=1, shuffle=False)\n",
    "finetune_model(model, finetune_dataloader, criterion, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52418c8-205b-4de4-a033-f63f9619b1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_models(model,\"ch12_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35346d9-1740-4ac8-8bd8-7c297b3e59c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model(dataloader, modelobj):\n",
    "    test_running_loss = 0.0\n",
    "    test_total_samples = 0\n",
    "    test_absolute_errors = 0.0\n",
    "    predicted_values = []\n",
    "    actual_values=[]\n",
    "    \n",
    "    # Switch model to evaluation mode\n",
    "    modelobj.eval()\n",
    "    \n",
    "    with torch.no_grad():  # No need to compute gradients during testing\n",
    "        for data in dataloader:\n",
    "            inputs, labels = data\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = modelobj(inputs)\n",
    "            predicted_values.append(outputs)\n",
    "            actual_values.append(labels)\n",
    "    \n",
    "            # Calculate loss (MSE or MAE)\n",
    "            test_loss = criterion(outputs, labels)\n",
    "            test_running_loss += test_loss.item()\n",
    "            \n",
    "            # Calculate absolute error (MAE)\n",
    "            test_absolute_error = torch.abs(outputs - labels)\n",
    "            test_absolute_errors += test_absolute_error.sum().item()\n",
    "            \n",
    "            # Update total samples\n",
    "            test_total_samples += labels.size(0)\n",
    "    \n",
    "    # Calculate Mean Absolute Error (MAE) on test dataset\n",
    "    test_mean_absolute_error = test_absolute_errors / test_total_samples\n",
    "    \n",
    "    # Print loss and MAE on test dataset\n",
    "    # print(f'Test Loss: {test_running_loss/len(test_dataloader)}, Test MAE: {test_mean_absolute_error}')\n",
    "    \n",
    "    predicted_list = [round(output.item(),3) for output in predicted_values]\n",
    "    actual_list = [round(output.item(),3) for output in actual_values]\n",
    "    \n",
    "    return np.array(actual_list), np.array(predicted_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfc6dce-f27b-4121-8108-d9026ec6f3cf",
   "metadata": {},
   "source": [
    "# Preparing testing data and train DLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8318c4dc-17ce-4f45-9fd5-cba148d8ee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test_data)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "y_true_test, y_pred_test=predict_model(test_dataloader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bddb7b-5cdf-4d70-b679-ac252f528f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_df = test_data[['features']].copy()\n",
    "reference_df['y_pred'] = y_pred_test \n",
    "reference_df['y_true'] = y_true_test \n",
    "\n",
    "# Creating features from the list\n",
    "values = reference_df['features'].iloc[0]\n",
    "for i, value in enumerate(values[::1], start=1):\n",
    "    reference_df[f'lag_{9-i}'] = value\n",
    "reference_df.drop(columns=['features'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea909aa-17fb-4de3-847c-cf418659dab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dle(reference_df):\n",
    "    features=['lag_8', 'lag_7', 'lag_6','lag_5','lag_4', 'lag_3', 'lag_2', 'lag_1']\n",
    "    dle = nml.DLE(\n",
    "        metrics=['mse'],\n",
    "        y_true='y_true',\n",
    "        y_pred='y_pred',\n",
    "        feature_column_names=features, chunk_size=150)\n",
    "    dle.fit(reference_df) # fit on the reference dataset\n",
    "    return dle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d911ce-1eb3-4eca-8cb8-6bc1efd89d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "dle=train_dle(reference_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444a3781-8c0b-48da-a60a-083cd7fbe527",
   "metadata": {},
   "source": [
    "# Model Monitoring in Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24033440-18f1-4901-b019-c472996a5777",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flow=prod_data.reset_index(drop=True)\n",
    "\n",
    "# Number of records in each batch\n",
    "batch_size = 1000\n",
    "\n",
    "# Calculate the total number of batches needed\n",
    "total_batches = int(np.ceil(len(data_flow) / batch_size))\n",
    "\n",
    "# Create the batch numbers\n",
    "batch_numbers = np.repeat(range(1, total_batches + 1), batch_size)[:len(data_flow)]\n",
    "\n",
    "# Add the batch numbers as a new column\n",
    "data_flow['batch_number'] = batch_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce0883a-b7dc-4ce6-bf99-13463719c876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_latest_model():\n",
    "    # Initialize the MLflow client\n",
    "    client = MlflowClient()\n",
    "    model_name = \"Ch12Models\"\n",
    "    \n",
    "    # Get all versions of the model\n",
    "    all_versions = client.get_registered_model(name=model_name).latest_versions\n",
    "\n",
    "    # Find the latest version based on last updated timestamp\n",
    "    latest_version = max(all_versions, key=lambda version: version.last_updated_timestamp)\n",
    "\n",
    "    # Construct the model URI\n",
    "    model_uri = f\"models:/{model_name}/{latest_version.version}\"\n",
    "\n",
    "    # Load the model\n",
    "    model = mlflow.pytorch.load_model(model_uri)\n",
    "    # print(f\"Loaded model version: {latest_version.version}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ea68d6-cf60-4234-ba45-829f721195d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting new batch\n",
    "def predict_batch(batch_data, modelobj):\n",
    "    predicted_values = []\n",
    "    \n",
    "    features_list=batch_data['features'].tolist()\n",
    "    tensors = [torch.tensor([feature]) for feature in features_list]\n",
    "\n",
    "    modelobj.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs in tensors:\n",
    "            outputs = modelobj(inputs)\n",
    "            predicted_values.append(outputs)\n",
    "    predicted_list = [output.item() for output in predicted_values]\n",
    "    # predicted_list = [round(output.item(),3) for output in predicted_values]\n",
    "    \n",
    "    features_values = batch_data['features'].iloc[0]\n",
    "    for i, value in enumerate(features_values[::1], start=1):\n",
    "        batch_data[f'lag_{9-i}'] = value\n",
    "    # batch_data.drop(columns=['features'], inplace=True)\n",
    "    batch_data['y_pred']=predicted_list\n",
    "    \n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5106c2-1f8c-43af-accf-8e9706bcc1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_models = mlflow.search_runs(experiment_names=[\"Chapter-12\"])\n",
    "\n",
    "# for run_id in all_models[\"run_id\"]:\n",
    "#     model_uri = f\"runs:/{run_id}/finetuned_model\"\n",
    "#     print(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91403351-2941-41c8-8207-bb88add04efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparing_model(batch_data, latest_model_mae):\n",
    "    features_list=batch_data['features'].tolist()\n",
    "    tensors = [torch.tensor([feature]) for feature in features_list]\n",
    "    \n",
    "    # Retrieve all models from the model registry\n",
    "    all_models = mlflow.search_runs(experiment_names=[\"Chapter-12\"])\n",
    "    \n",
    "    mae_dict = {}\n",
    "    for run_id in all_models[\"run_id\"]:\n",
    "        predicted_values = []\n",
    "        model_uri = f\"runs:/{run_id}/ch12_model\"\n",
    "        model = mlflow.pytorch.load_model(model_uri)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs in tensors:\n",
    "                outputs = model(inputs)\n",
    "                predicted_values.append(outputs)\n",
    "        predicted_list = [output.item() for output in predicted_values]\n",
    "        \n",
    "        mae = mean_absolute_error(batch_data['target'], predicted_list)\n",
    "        mae_dict[model_uri] = mae\n",
    "\n",
    "    # Select the best performing model\n",
    "    best_model_uri = min(mae_dict, key=mae_dict.get)\n",
    "\n",
    "    # Benchmark the model on the current batch of data if its performance is lower than that of the best model retrieved from the model registry.\n",
    "    if mae_dict[best_model_uri]<latest_model_mae:\n",
    "        best_model = mlflow.pytorch.load_model(best_model_uri)\n",
    "        batch_dataset = CustomDataset(batch_data)\n",
    "        batch_dataloader = DataLoader(batch_dataset, batch_size=1, shuffle=False)\n",
    "        finetune_model(best_model, batch_dataloader, criterion, optimizer, epochs)\n",
    "        log_models(best_model,\"ch12_model\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d74e3c4-73d9-4d5e-bd21-611d9f8789c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest_model=load_latest_model()\n",
    "# for m in range(1,2):\n",
    "#     p=data_flow.query('batch_number==@m')\n",
    "#     # analysis_df=predict_batch(p.copy(), latest_model)\n",
    "#     comparing_model(p.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2655a2f-7fd2-4234-8b80-f39586938470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-by-step process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9449c6-001d-4973-9833-0427e3b21e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp=0 # \n",
    "flag=False #jsut for tracking \n",
    "predicted_df=pd.DataFrame()\n",
    "benchmark_df=pd.DataFrame()\n",
    "\n",
    "latest_model=load_latest_model() # calling current model\n",
    "for batch_number in range(1, total_batches+1):\n",
    "    data=data_flow.query('batch_number==@batch_number')\n",
    "    analysis_df=predict_batch(copy.deepcopy(data), latest_model) # Need to explain as it is very important\n",
    "    predicted_df=predicted_df.append(analysis_df,ignore_index=True) # Just to store all inputs, batches and predicted results \n",
    "\n",
    "    #### need to update dle on previous batches because for current data GT is not available\n",
    "    \n",
    "    estimated_performance = dle.estimate(copy.deepcopy(analysis_df)) # estimate on the current batch of data\n",
    "    drift=estimated_performance.to_df()['mse']['alert']\n",
    "    \n",
    "    \n",
    "    # Benchmark the model on the current batch of data if its performance is lower than that of the best model retrieved from the model registry\n",
    "    # if(temp>batch_number):\n",
    "    if(flag==True):\n",
    "        latest_model_mae = mean_absolute_error(benchmark_df['target'], benchmark_df['y_pred'])\n",
    "        comparing_model(copy.deepcopy(benchmark_df), latest_model_mae)\n",
    "        flag=False\n",
    "                       \n",
    "    \n",
    "    # Action if Drift is detected in current batch\n",
    "    # remove do from do_benchmark\n",
    "    if drift.any():\n",
    "        benchmark_df=copy.deepcopy(analysis_df)\n",
    "        flag=True\n",
    "        # temp=batch_number+2\n",
    "    predicted_df['y_true']=predicted_df['target']\n",
    "    dle=train_dle(predicted_df.tail(batch_size*5)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
